{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic: Hw5, Machine-Learning-2019-fall\n",
    "#### Date: 2019-11-21\n",
    "#### Author: 林俊儒(B063040058), 王譽鈞(B065040034)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 作業要求:\n",
    "##### 利用 tensorflow 或 keras 撰寫神經網路模型訓練 Cifar10 資料集。\n",
    "##### 請調整下列各種設定來比較訓練結果之好壞。\n",
    "---\n",
    "##### !!! 下面這個import用的 cell，有時候跑第一次會有wairning，要多跑幾次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Add, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, SeparableConv2D\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 大量運算時間注意：\n",
    "##### 因為有非常多測試的case \n",
    "##### 如果一口氣跑  所有的Cell 會跑非常久～～\n",
    "##### 建議可以一個一個看狀況跑\n",
    "##### 另外，我們有把我們自己在電腦測試時的結果\n",
    "##### 截圖 放在註解的cell\n",
    "##### 程式還在跑的時候也可以先參考註解的截圖\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 測試0: 比較用的-原始架構\n",
    "##### 用來跟其他調整後的架構比較用的原始架構，\n",
    "##### 後面的case都是以這個為基礎去修改比較\n",
    "##### 架構如下: -------------------------------------------------\n",
    "##### 先兩層 convolution(kernel:3x3,filter:32,padding='same)\n",
    "##### 接一層 maxPooling(2,2)\n",
    "##### 再兩層 convolution(kernel:3x3,filter:64,padding='same)\n",
    "##### 接一層 maxPooling(2,2)\n",
    "##### 然後flatten後\n",
    "##### 接一層 全連接的 Dense(units=512)\n",
    "##### 然後最後面 接output的10類別: Dense(units=10)\n",
    "##### -----------------------------------------------------------\n",
    "##### epochs = 50\n",
    "##### batch_size = 100\n",
    "##### activation 最後一層output類別 使用 softmax\n",
    "##### activation 其他所有層 使用 relu\n",
    "##### optimizer 使用 adam (參數使用預設值)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4115: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 2.2789 - acc: 0.1360 - val_loss: 2.1846 - val_acc: 0.2200\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.1099 - acc: 0.2450 - val_loss: 2.0542 - val_acc: 0.2640\n",
      "Epoch 3/10\n",
      " - 2s - loss: 1.9519 - acc: 0.3130 - val_loss: 1.9141 - val_acc: 0.2920\n",
      "Epoch 4/10\n",
      " - 2s - loss: 1.7901 - acc: 0.3760 - val_loss: 1.7832 - val_acc: 0.3200\n",
      "Epoch 5/10\n",
      " - 2s - loss: 1.6567 - acc: 0.3980 - val_loss: 1.7223 - val_acc: 0.3200\n",
      "Epoch 6/10\n",
      " - 2s - loss: 1.5232 - acc: 0.4710 - val_loss: 1.7404 - val_acc: 0.3920\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.4444 - acc: 0.4730 - val_loss: 1.6828 - val_acc: 0.4040\n",
      "Epoch 8/10\n",
      " - 2s - loss: 1.2966 - acc: 0.5270 - val_loss: 1.7233 - val_acc: 0.3800\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.2108 - acc: 0.5880 - val_loss: 1.7196 - val_acc: 0.4240\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.0380 - acc: 0.6330 - val_loss: 1.6204 - val_acc: 0.4520\n",
      "1000/1000 [==============================] - 0s 497us/step\n",
      "[Info] Accuracy of testing data = 40.800%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "cmp_origin_score = scores[1]*100.0\n",
    "#print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/gAfPWAD.png)\n",
    "##### 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "\n",
    "---\n",
    "#### 測試一：神經網路之深度\n",
    "##### 我們將convlution後的全連接的層\n",
    "##### 除了原本的 Dense(units=512)\n",
    "##### 再多接二層 Dense(units=256) 和 Dense(units=128)\n",
    "##### 最後面一樣接output的10類別: Dense(units=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 2.2811 - acc: 0.1200 - val_loss: 2.2216 - val_acc: 0.1040\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.1616 - acc: 0.2010 - val_loss: 2.0822 - val_acc: 0.2320\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.0086 - acc: 0.2560 - val_loss: 1.9368 - val_acc: 0.2400\n",
      "Epoch 4/10\n",
      " - 2s - loss: 1.9132 - acc: 0.3010 - val_loss: 1.9080 - val_acc: 0.3160\n",
      "Epoch 5/10\n",
      " - 2s - loss: 1.8365 - acc: 0.3380 - val_loss: 1.7858 - val_acc: 0.3560\n",
      "Epoch 6/10\n",
      " - 2s - loss: 1.7053 - acc: 0.3740 - val_loss: 1.7493 - val_acc: 0.3320\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.6174 - acc: 0.3980 - val_loss: 1.6840 - val_acc: 0.3480\n",
      "Epoch 8/10\n",
      " - 2s - loss: 1.5221 - acc: 0.4450 - val_loss: 1.7160 - val_acc: 0.3320\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.4071 - acc: 0.4760 - val_loss: 1.6599 - val_acc: 0.3800\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.2962 - acc: 0.5210 - val_loss: 1.5286 - val_acc: 0.4560\n",
      "1000/1000 [==============================] - 0s 453us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 41.600%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=256, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=128, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/eqrAImh.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/yFL52nB.png)\n",
    "\n",
    "##### 測試結果顯示準確率差異不大：\n",
    "---\n",
    "#### 測試二：dropout 之使用與否\n",
    "##### 我們將convlution層 每二層 dropout 75% (共2次)\n",
    "##### 後面的Dense層 dropout 50% 再接最後分類輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 2.3115 - acc: 0.0820 - val_loss: 2.2864 - val_acc: 0.1320\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.2669 - acc: 0.1320 - val_loss: 2.2122 - val_acc: 0.1760\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.1210 - acc: 0.2230 - val_loss: 1.9876 - val_acc: 0.2440\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.0317 - acc: 0.2360 - val_loss: 1.9854 - val_acc: 0.2800\n",
      "Epoch 5/10\n",
      " - 2s - loss: 1.9359 - acc: 0.2960 - val_loss: 1.8807 - val_acc: 0.3000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 1.8649 - acc: 0.3220 - val_loss: 1.8446 - val_acc: 0.2840\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.7861 - acc: 0.3370 - val_loss: 1.8093 - val_acc: 0.3120\n",
      "Epoch 8/10\n",
      " - 2s - loss: 1.6945 - acc: 0.3890 - val_loss: 1.8101 - val_acc: 0.3200\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.6235 - acc: 0.4010 - val_loss: 1.6408 - val_acc: 0.3920\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.5919 - acc: 0.4200 - val_loss: 1.6875 - val_acc: 0.3600\n",
      "1000/1000 [==============================] - 0s 443us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 37.500%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "out_x = Dropout(0.25)(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "out_x = Dropout(0.25)(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dropout(0.5)(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/AhGASs8.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/q1GQBeC.png)\n",
    "\n",
    "##### 測試結果顯示 適當dropout 有助提昇準確率\n",
    "---\n",
    "#### 測試三：batch normalization 之使用與否\n",
    "##### 我們將convlution層 每一層的輸出 都normalize(共4次)\n",
    "##### 後面的Dense層 也normalize後 再接最後分類輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/chunrulin/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 7s - loss: 2.1763 - acc: 0.2700 - val_loss: 2.3151 - val_acc: 0.2920\n",
      "Epoch 2/10\n",
      " - 4s - loss: 1.0881 - acc: 0.6330 - val_loss: 1.9790 - val_acc: 0.4120\n",
      "Epoch 3/10\n",
      " - 5s - loss: 0.6516 - acc: 0.8130 - val_loss: 1.9355 - val_acc: 0.3840\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.3800 - acc: 0.9310 - val_loss: 1.9084 - val_acc: 0.4400\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.1852 - acc: 0.9850 - val_loss: 1.9367 - val_acc: 0.3840\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.1039 - acc: 0.9950 - val_loss: 1.7704 - val_acc: 0.4200\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.0527 - acc: 1.0000 - val_loss: 1.8205 - val_acc: 0.4000\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.0346 - acc: 1.0000 - val_loss: 1.8624 - val_acc: 0.3960\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0219 - acc: 1.0000 - val_loss: 1.8909 - val_acc: 0.3960\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.0168 - acc: 1.0000 - val_loss: 1.8887 - val_acc: 0.4280\n",
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 42.200%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/UQqXGhk.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/j4UPWzz.png)\n",
    "\n",
    "##### 測試結果顯示 適當BatchNormalization 可提昇一點準確率\n",
    "---\n",
    "#### 測試四：不同 activation function 之使用\n",
    "##### 除了最後分類輸出 固定使用 softmax\n",
    "##### 其他都統一改成不同的來嘗試\n",
    "##### 我們測了 4種case\n",
    "##### 測試4-1：activation 使用 elu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 2.4176 - acc: 0.1680 - val_loss: 2.0205 - val_acc: 0.1760\n",
      "Epoch 2/10\n",
      " - 2s - loss: 1.8720 - acc: 0.3190 - val_loss: 1.8008 - val_acc: 0.3360\n",
      "Epoch 3/10\n",
      " - 2s - loss: 1.6747 - acc: 0.3950 - val_loss: 1.7979 - val_acc: 0.3520\n",
      "Epoch 4/10\n",
      " - 2s - loss: 1.5162 - acc: 0.4740 - val_loss: 1.6811 - val_acc: 0.3960\n",
      "Epoch 5/10\n",
      " - 3s - loss: 1.3730 - acc: 0.5160 - val_loss: 1.6205 - val_acc: 0.4040\n",
      "Epoch 6/10\n",
      " - 2s - loss: 1.2192 - acc: 0.5910 - val_loss: 1.6252 - val_acc: 0.4040\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.0667 - acc: 0.6450 - val_loss: 1.6941 - val_acc: 0.4200\n",
      "Epoch 8/10\n",
      " - 3s - loss: 0.9550 - acc: 0.6820 - val_loss: 1.7952 - val_acc: 0.4160\n",
      "Epoch 9/10\n",
      " - 3s - loss: 0.8041 - acc: 0.7380 - val_loss: 1.8179 - val_acc: 0.4320\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.6902 - acc: 0.7740 - val_loss: 1.9116 - val_acc: 0.4000\n",
      "1000/1000 [==============================] - 1s 660us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 40.300%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('elu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('elu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('elu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('elu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='elu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/0wVBTEs.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/neeH5Ni.png)\n",
    "\n",
    "##### 測試結果顯示 elu 對於這次的資料集與分類目標 沒有提昇準確率\n",
    "---\n",
    "##### 測試4-2：activation 使用 linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 2.7071 - acc: 0.1390 - val_loss: 2.1345 - val_acc: 0.2040\n",
      "Epoch 2/10\n",
      " - 1s - loss: 1.9918 - acc: 0.2760 - val_loss: 1.9199 - val_acc: 0.2960\n",
      "Epoch 3/10\n",
      " - 2s - loss: 1.7767 - acc: 0.3730 - val_loss: 1.8318 - val_acc: 0.3320\n",
      "Epoch 4/10\n",
      " - 1s - loss: 1.6372 - acc: 0.4300 - val_loss: 1.7217 - val_acc: 0.3800\n",
      "Epoch 5/10\n",
      " - 1s - loss: 1.5705 - acc: 0.4590 - val_loss: 1.7071 - val_acc: 0.4280\n",
      "Epoch 6/10\n",
      " - 1s - loss: 1.4235 - acc: 0.5170 - val_loss: 1.6779 - val_acc: 0.4120\n",
      "Epoch 7/10\n",
      " - 1s - loss: 1.3225 - acc: 0.5560 - val_loss: 1.6340 - val_acc: 0.4280\n",
      "Epoch 8/10\n",
      " - 1s - loss: 1.2099 - acc: 0.6040 - val_loss: 1.6281 - val_acc: 0.4280\n",
      "Epoch 9/10\n",
      " - 1s - loss: 1.1167 - acc: 0.6210 - val_loss: 1.7227 - val_acc: 0.4160\n",
      "Epoch 10/10\n",
      " - 1s - loss: 1.0364 - acc: 0.6520 - val_loss: 1.7976 - val_acc: 0.4280\n",
      "1000/1000 [==============================] - 0s 431us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 38.800%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('linear')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('linear')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('linear')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('linear')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='linear')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/At71EJG.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/ZJ41DYu.png)\n",
    "\n",
    "##### 測試結果顯示 linear 對於這次的資料集與分類目標 準確率反而明顯下降\n",
    "---\n",
    "##### 測試4-3：activation 使用 sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 2.6052 - acc: 0.1040 - val_loss: 2.3740 - val_acc: 0.0960\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.3612 - acc: 0.1090 - val_loss: 2.3052 - val_acc: 0.0680\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.3288 - acc: 0.0960 - val_loss: 2.3227 - val_acc: 0.1320\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.3181 - acc: 0.1100 - val_loss: 2.3106 - val_acc: 0.1040\n",
      "Epoch 5/10\n",
      " - 2s - loss: 2.3223 - acc: 0.0900 - val_loss: 2.3231 - val_acc: 0.0880\n",
      "Epoch 6/10\n",
      " - 2s - loss: 2.3060 - acc: 0.1040 - val_loss: 2.3032 - val_acc: 0.1320\n",
      "Epoch 7/10\n",
      " - 2s - loss: 2.3096 - acc: 0.1100 - val_loss: 2.3079 - val_acc: 0.1360\n",
      "Epoch 8/10\n",
      " - 2s - loss: 2.3137 - acc: 0.0970 - val_loss: 2.3113 - val_acc: 0.0680\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.3137 - acc: 0.1110 - val_loss: 2.3042 - val_acc: 0.1320\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3131 - acc: 0.0990 - val_loss: 2.3135 - val_acc: 0.1000\n",
      "1000/1000 [==============================] - 1s 592us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 10.600%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='sigmoid')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/qoFC9Ap.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/g4UTu7G.png)\n",
    "\n",
    "##### 測試結果顯示 sigmoid 對於這次的資料集與分類目標 準確率反而明顯下降\n",
    "---\n",
    "##### 測試4-4：activation 使用 sigmoid，再搭配之前的 batch-normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 6s - loss: 2.1509 - acc: 0.2970 - val_loss: 8.9077 - val_acc: 0.1720\n",
      "Epoch 2/10\n",
      " - 4s - loss: 1.4339 - acc: 0.4990 - val_loss: 4.4994 - val_acc: 0.2000\n",
      "Epoch 3/10\n",
      " - 4s - loss: 1.0857 - acc: 0.6470 - val_loss: 6.1503 - val_acc: 0.1800\n",
      "Epoch 4/10\n",
      " - 4s - loss: 0.8280 - acc: 0.7450 - val_loss: 7.6437 - val_acc: 0.1320\n",
      "Epoch 5/10\n",
      " - 4s - loss: 0.5873 - acc: 0.8610 - val_loss: 3.8785 - val_acc: 0.2560\n",
      "Epoch 6/10\n",
      " - 4s - loss: 0.3929 - acc: 0.9320 - val_loss: 3.6475 - val_acc: 0.2480\n",
      "Epoch 7/10\n",
      " - 4s - loss: 0.2305 - acc: 0.9840 - val_loss: 5.2438 - val_acc: 0.1720\n",
      "Epoch 8/10\n",
      " - 4s - loss: 0.1266 - acc: 0.9970 - val_loss: 2.6132 - val_acc: 0.2840\n",
      "Epoch 9/10\n",
      " - 4s - loss: 0.0780 - acc: 0.9980 - val_loss: 2.4439 - val_acc: 0.3480\n",
      "Epoch 10/10\n",
      " - 4s - loss: 0.0451 - acc: 1.0000 - val_loss: 3.0762 - val_acc: 0.2600\n",
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 28.500%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('sigmoid')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='sigmoid')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/rAewSdd.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/5pmeSzv.png)\n",
    "\n",
    "##### 測試結果顯示，batch-normalization 能幫助將資料標準化到 sigmoid 適合的區間\n",
    "##### 因此 sigmoid 在訓練時準確率有較4-3提昇\n",
    "##### 但目前的測試結果 還沒有明顯比原始架構好的結果\n",
    "---\n",
    "#### 測試五：不同訓練方法及 learning rate 之設定\n",
    "##### 我們測了 5種case\n",
    "##### 測試5-1: Adam的learning rate改為0.01 (原本預設值 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 3.1511 - acc: 0.1210 - val_loss: 2.2892 - val_acc: 0.1520\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.3015 - acc: 0.1120 - val_loss: 2.3051 - val_acc: 0.1320\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.3032 - acc: 0.1070 - val_loss: 2.3045 - val_acc: 0.1320\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.3025 - acc: 0.1050 - val_loss: 2.3074 - val_acc: 0.0680\n",
      "Epoch 5/10\n",
      " - 2s - loss: 2.3009 - acc: 0.1120 - val_loss: 2.3052 - val_acc: 0.0680\n",
      "Epoch 6/10\n",
      " - 2s - loss: 2.3017 - acc: 0.1120 - val_loss: 2.3091 - val_acc: 0.0680\n",
      "Epoch 7/10\n",
      " - 2s - loss: 2.3017 - acc: 0.1120 - val_loss: 2.3030 - val_acc: 0.0680\n",
      "Epoch 8/10\n",
      " - 2s - loss: 2.3014 - acc: 0.1120 - val_loss: 2.3044 - val_acc: 0.0680\n",
      "Epoch 9/10\n",
      " - 2s - loss: 2.3009 - acc: 0.1120 - val_loss: 2.3054 - val_acc: 0.0680\n",
      "Epoch 10/10\n",
      " - 2s - loss: 2.3014 - acc: 0.1120 - val_loss: 2.3079 - val_acc: 0.0680\n",
      "1000/1000 [==============================] - 0s 411us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 8.900%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "my_opt = keras.optimizers.Adam(lr=0.01)\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer=my_opt, metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/DrH3HcQ.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/XwMDQ3V.png)\n",
    "\n",
    "##### 測試結果顯示，adam learning rate 提昇成0.01之後非常悽慘\n",
    "---\n",
    "##### 測試5-2: Adam的learning rate改為0.0001 (原本預設值 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 2.2897 - acc: 0.1190 - val_loss: 2.2653 - val_acc: 0.1520\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.2529 - acc: 0.1710 - val_loss: 2.2280 - val_acc: 0.2280\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.2094 - acc: 0.2280 - val_loss: 2.1771 - val_acc: 0.2920\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.1605 - acc: 0.2640 - val_loss: 2.1373 - val_acc: 0.2800\n",
      "Epoch 5/10\n",
      " - 2s - loss: 2.0956 - acc: 0.2880 - val_loss: 2.0582 - val_acc: 0.3000\n",
      "Epoch 6/10\n",
      " - 2s - loss: 2.0156 - acc: 0.3080 - val_loss: 1.9930 - val_acc: 0.3120\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.9656 - acc: 0.3100 - val_loss: 1.9850 - val_acc: 0.2920\n",
      "Epoch 8/10\n",
      " - 2s - loss: 1.8972 - acc: 0.3340 - val_loss: 1.9124 - val_acc: 0.3240\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.8230 - acc: 0.3810 - val_loss: 1.8915 - val_acc: 0.3160\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.7833 - acc: 0.3780 - val_loss: 1.8385 - val_acc: 0.3400\n",
      "1000/1000 [==============================] - 0s 416us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 33.500%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "my_opt = keras.optimizers.Adam(lr=0.0001)\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer=my_opt, metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/6wGzsZb.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/GNe9VaW.png)\n",
    "\n",
    "##### 測試結果顯示，adam learning rate 下降成0.0001之後，較原始架構的準確度下降一點\n",
    "---\n",
    "##### 測試5-3:改為使用 RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 2.3354 - acc: 0.1200 - val_loss: 2.2414 - val_acc: 0.1680\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.2889 - acc: 0.1670 - val_loss: 2.1689 - val_acc: 0.1760\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.1591 - acc: 0.2410 - val_loss: 1.9817 - val_acc: 0.3040\n",
      "Epoch 4/10\n",
      " - 2s - loss: 1.9841 - acc: 0.2860 - val_loss: 2.2061 - val_acc: 0.2120\n",
      "Epoch 5/10\n",
      " - 2s - loss: 1.8840 - acc: 0.3400 - val_loss: 1.8139 - val_acc: 0.3360\n",
      "Epoch 6/10\n",
      " - 2s - loss: 1.7753 - acc: 0.3800 - val_loss: 1.7480 - val_acc: 0.3440\n",
      "Epoch 7/10\n",
      " - 2s - loss: 1.8117 - acc: 0.3860 - val_loss: 1.7563 - val_acc: 0.3520\n",
      "Epoch 8/10\n",
      " - 2s - loss: 1.6021 - acc: 0.4470 - val_loss: 2.2531 - val_acc: 0.2480\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.5423 - acc: 0.4740 - val_loss: 1.7173 - val_acc: 0.3920\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.4258 - acc: 0.5020 - val_loss: 1.6508 - val_acc: 0.3560\n",
      "1000/1000 [==============================] - 0s 441us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 37.200%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/2wCJfg3.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/F3gN5Wz.png)\n",
    "\n",
    "##### 測試結果顯示，RMSprop，準確度和原始架構差不多\n",
    "---\n",
    "##### 測試5-4:改為使用 Adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 3.4915 - acc: 0.0900 - val_loss: 2.3041 - val_acc: 0.0680\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.4175 - acc: 0.1190 - val_loss: 2.2935 - val_acc: 0.1040\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.3142 - acc: 0.0990 - val_loss: 2.3056 - val_acc: 0.0680\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.2978 - acc: 0.1180 - val_loss: 2.2894 - val_acc: 0.0840\n",
      "Epoch 5/10\n",
      " - 2s - loss: 2.2816 - acc: 0.1440 - val_loss: 2.2703 - val_acc: 0.1880\n",
      "Epoch 6/10\n",
      " - 2s - loss: 2.4434 - acc: 0.1710 - val_loss: 2.2812 - val_acc: 0.1280\n",
      "Epoch 7/10\n",
      " - 2s - loss: 2.2271 - acc: 0.1760 - val_loss: 2.1398 - val_acc: 0.2320\n",
      "Epoch 8/10\n",
      " - 1s - loss: 2.0305 - acc: 0.2670 - val_loss: 2.1254 - val_acc: 0.2160\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.9240 - acc: 0.3150 - val_loss: 2.0891 - val_acc: 0.2280\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.8710 - acc: 0.3400 - val_loss: 1.8751 - val_acc: 0.3320\n",
      "1000/1000 [==============================] - 0s 451us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 33.700%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='Adagrad', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/cca5qfl.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/B0iFCrY.png)\n",
    "\n",
    "##### 測試結果顯示，Adagrad，準確度也和原始架構差不多\n",
    "---\n",
    "##### 測試5-5:改為使用 SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 3s - loss: 2.3022 - acc: 0.1080 - val_loss: 2.3011 - val_acc: 0.1440\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.2970 - acc: 0.1320 - val_loss: 2.2986 - val_acc: 0.1320\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.2931 - acc: 0.1270 - val_loss: 2.2965 - val_acc: 0.1080\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.2903 - acc: 0.1300 - val_loss: 2.2942 - val_acc: 0.0920\n",
      "Epoch 5/10\n",
      " - 2s - loss: 2.2875 - acc: 0.1390 - val_loss: 2.2922 - val_acc: 0.0880\n",
      "Epoch 6/10\n",
      " - 1s - loss: 2.2854 - acc: 0.1260 - val_loss: 2.2901 - val_acc: 0.0840\n",
      "Epoch 7/10\n",
      " - 1s - loss: 2.2825 - acc: 0.1320 - val_loss: 2.2878 - val_acc: 0.0840\n",
      "Epoch 8/10\n",
      " - 2s - loss: 2.2794 - acc: 0.1440 - val_loss: 2.2851 - val_acc: 0.0960\n",
      "Epoch 9/10\n",
      " - 1s - loss: 2.2767 - acc: 0.1480 - val_loss: 2.2825 - val_acc: 0.1240\n",
      "Epoch 10/10\n",
      " - 1s - loss: 2.2730 - acc: 0.1470 - val_loss: 2.2791 - val_acc: 0.1360\n",
      "1000/1000 [==============================] - 0s 429us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 14.800%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/jUjDhK3.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/iz9STPN.png)\n",
    "\n",
    "##### 測試結果顯示，SGD，準確度明顯較原始架構的差一些\n",
    "---\n",
    "### 測試六：Residual 架構之使用\n",
    "##### 我們將convlution層 每二層的輸出後面\n",
    "##### 都再接一層 Residual add (共2次)\n",
    "##### 然後就跟原本的一樣flatten後全連接dense(units=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape: (32, 32, 3)\n",
      "residual_shape: (32, 32, 32)\n",
      "input_shape: (32, 32, 32)\n",
      "residual_shape: (16, 16, 64)\n",
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 5s 5ms/step - loss: 2.3342 - acc: 0.1570 - val_loss: 2.1816 - val_acc: 0.1880\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 2.0528 - acc: 0.2860 - val_loss: 1.9821 - val_acc: 0.2920\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.8499 - acc: 0.3630 - val_loss: 1.8056 - val_acc: 0.3520\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.6718 - acc: 0.4070 - val_loss: 1.6630 - val_acc: 0.4120\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.5441 - acc: 0.4440 - val_loss: 1.6064 - val_acc: 0.4400\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.4024 - acc: 0.5230 - val_loss: 1.6125 - val_acc: 0.4200\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 1.2891 - acc: 0.5420 - val_loss: 1.6192 - val_acc: 0.4280\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.1232 - acc: 0.6080 - val_loss: 1.6398 - val_acc: 0.4040\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 3s 3ms/step - loss: 1.0015 - acc: 0.6580 - val_loss: 1.9071 - val_acc: 0.3960\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 2s 2ms/step - loss: 0.9816 - acc: 0.6590 - val_loss: 1.6911 - val_acc: 0.4400\n",
      "1000/1000 [==============================] - 1s 544us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 41.000%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "#data_augmentation = True\n",
    "#num_predictions = 20\n",
    "#model_name = 'keras_cifar10_trained_model.h5'\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "\n",
    "#count for residual parameter\n",
    "input_shape = add_x.shape[1:]\n",
    "residual_shape = out_x.shape[1:]\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"residual_shape:\", residual_shape)\n",
    "cur_channels = (int)(residual_shape[2])\n",
    "stride_width = int(input_shape[0]) // int(residual_shape[0])\n",
    "stride_height = int(input_shape[1]) // int(residual_shape[1])\n",
    "add_x = Conv2D(filters=cur_channels, kernel_size=(1, 1), strides=(stride_width, stride_height), padding='valid')(add_x)\n",
    "out_x = Add()([out_x,add_x])\n",
    "\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "\n",
    "#count for residual parameter\n",
    "input_shape = add_x.shape[1:]\n",
    "residual_shape = out_x.shape[1:]\n",
    "print(\"input_shape:\", input_shape)\n",
    "print(\"residual_shape:\", residual_shape)\n",
    "cur_channels = (int)(residual_shape[2])\n",
    "stride_width = int(input_shape[0]) // int(residual_shape[0])\n",
    "stride_height = int(input_shape[1]) // int(residual_shape[1])\n",
    "add_x = Conv2D(filters=cur_channels, kernel_size=(1, 1), strides=(stride_width, stride_height), padding='valid')(add_x)\n",
    "out_x = Add()([out_x,add_x])\n",
    "\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=1)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/OJDscU1.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/gqnLHww.png)\n",
    "\n",
    "##### 測試結果顯示 以這次較簡單的CNN架構來說\n",
    "##### 簡單的Residual 架構 對準確率沒有太大的影響\n",
    "---\n",
    "### 測試七：Depthwise convolution 之使用\n",
    "##### 因為 Depthwise 通常會搭配 pointwise\n",
    "##### 因此我們直接使用keras 內建的 SeparableConv2D\n",
    "##### 將我們原本的 convolution層 都替換成 SeparableConv2D (共替換4層)\n",
    "##### 然後就跟原本的一樣flatten後全連接dense(units=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      " - 4s - loss: 2.3034 - acc: 0.1050 - val_loss: 2.3051 - val_acc: 0.0680\n",
      "Epoch 2/10\n",
      " - 2s - loss: 2.3014 - acc: 0.1120 - val_loss: 2.3040 - val_acc: 0.0680\n",
      "Epoch 3/10\n",
      " - 2s - loss: 2.3002 - acc: 0.1120 - val_loss: 2.3068 - val_acc: 0.0680\n",
      "Epoch 4/10\n",
      " - 2s - loss: 2.3026 - acc: 0.1120 - val_loss: 2.3041 - val_acc: 0.0680\n",
      "Epoch 5/10\n",
      " - 2s - loss: 2.2986 - acc: 0.1120 - val_loss: 2.2978 - val_acc: 0.0680\n",
      "Epoch 6/10\n",
      " - 2s - loss: 2.2842 - acc: 0.1160 - val_loss: 2.2630 - val_acc: 0.1160\n",
      "Epoch 7/10\n",
      " - 2s - loss: 2.2133 - acc: 0.1680 - val_loss: 2.2053 - val_acc: 0.1360\n",
      "Epoch 8/10\n",
      " - 2s - loss: 2.1081 - acc: 0.2290 - val_loss: 2.0331 - val_acc: 0.1760\n",
      "Epoch 9/10\n",
      " - 2s - loss: 1.9839 - acc: 0.2740 - val_loss: 2.0539 - val_acc: 0.2440\n",
      "Epoch 10/10\n",
      " - 2s - loss: 1.9460 - acc: 0.2740 - val_loss: 1.9401 - val_acc: 0.2840\n",
      "1000/1000 [==============================] - 0s 410us/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 27.100%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = SeparableConv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = SeparableConv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512, kernel_initializer='normal', activation='relu')(out_x)\n",
    "out_x = Dense(units=10, kernel_initializer='normal', activation='softmax')(out_x)\n",
    "\n",
    "#### compile model ####\n",
    "#### compile model ####\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=2)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 測試時的截圖：\n",
    "![](https://i.imgur.com/QrWDWhu.png)\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 這次測試 準確率截圖：\n",
    "![](https://i.imgur.com/qza9IKB.png)\n",
    "\n",
    "##### Depthwise 和 pointwise 主要是用來減少計算量\n",
    "##### 若單獨使用，沒搭配其他方法\n",
    "##### 測試結果和預期的一樣，對準確率沒太大的幫助\n",
    "---\n",
    "#### 額外測試：嘗試找到最好的組合\n",
    "##### 我們在原始的架構下(測試0版本)\n",
    "##### 同時搭配 BatchNormalization 與 Dropout\n",
    "##### 並在後面flatten後，除了原本的 Dense(units=512)\n",
    "##### 再多接二層 Dense(units=256) 和 Dense(units=128)\n",
    "##### 最後面一樣接output的10類別: Dense(units=10)\n",
    "##### 最後把 epochs 拉高到 150 做fine tune測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 250 samples\n",
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 2.7830 - acc: 0.1080 - val_loss: 2.1248 - val_acc: 0.1840\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.5516 - acc: 0.1860 - val_loss: 1.9748 - val_acc: 0.3200\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.3747 - acc: 0.2050 - val_loss: 1.8979 - val_acc: 0.3200\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.1510 - acc: 0.2790 - val_loss: 1.8713 - val_acc: 0.3520\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 2.1273 - acc: 0.2650 - val_loss: 1.7855 - val_acc: 0.3160\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9654 - acc: 0.3160 - val_loss: 1.8712 - val_acc: 0.3120\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9304 - acc: 0.3190 - val_loss: 1.8123 - val_acc: 0.3400\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.9074 - acc: 0.3230 - val_loss: 1.8128 - val_acc: 0.3000\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.8182 - acc: 0.3370 - val_loss: 1.7224 - val_acc: 0.3360\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 4s 4ms/step - loss: 1.7077 - acc: 0.3910 - val_loss: 1.7480 - val_acc: 0.3280\n",
      "1000/1000 [==============================] - 1s 1ms/step\n",
      "[Info] Origin Architecture score = 40.800%\n",
      "[Info] Accuracy of testing data = 32.700%\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "my_epochs = 50\n",
    "my_batch_size = 100\n",
    "\n",
    "# The data, split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize data.\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "#### set input ####\n",
    "#### set input ####\n",
    "\n",
    "input_x = Input([32, 32, 3])\n",
    "add_x = input_x\n",
    "out_x = Reshape([32, 32, 3])(input_x)\n",
    "\n",
    "#### 1st-conv-2-layer ####\n",
    "#### 1st-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=32, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "out_x = Dropout(0.25)(out_x)\n",
    "\n",
    "#### 2nd-conv-2-layer ####\n",
    "#### 2nd-conv-2-layer ####\n",
    "\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Conv2D(filters=64, kernel_size=(3, 3), padding='same')(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = MaxPooling2D((2, 2))(out_x)\n",
    "out_x = Dropout(0.25)(out_x)\n",
    "\n",
    "#### final-NN-layer ####\n",
    "#### final-NN-layer ####\n",
    "\n",
    "out_x = Flatten()(out_x)\n",
    "out_x = Dense(units=512)(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Dropout(0.5)(out_x)\n",
    "out_x = Dense(units=256)(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Dropout(0.5)(out_x)\n",
    "out_x = Dense(units=128)(out_x)\n",
    "out_x = BatchNormalization()(out_x)\n",
    "out_x = Activation('relu')(out_x)\n",
    "out_x = Dropout(0.5)(out_x)\n",
    "out_x = Dense(units=10, activation='softmax')(out_x)\n",
    "\n",
    "my_model = Model(inputs=input_x,outputs=out_x)\n",
    "my_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "train_history = my_model.fit(x=x_train, y=y_train, validation_split=0.2, \\\n",
    "                             epochs=my_epochs, batch_size=my_batch_size, verbose=1)\n",
    "scores = my_model.evaluate(x_test, y_test)\n",
    "\n",
    "print(\"=====================================================\")\n",
    "print(\"[Info] Origin Architecture score = {:2.3f}%\".format(cmp_origin_score))\n",
    "print(\"[Info] Accuracy of testing data = {:2.3f}%\".format(scores[1]*100.0))\n",
    "print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##### 原始架構 準確率截圖：\n",
    "![](https://i.imgur.com/gardc1Q.png)\n",
    "##### 嘗試新組合架構 準確率截圖：\n",
    "![](https://i.imgur.com/cWmtF77.png)\n",
    "\n",
    "##### 測試後發現，適當的將各種方法組合應用，\n",
    "##### 能有效的提昇準確率一點\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
